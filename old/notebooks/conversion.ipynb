{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import shutil\n",
    "\n",
    "from scipy.sparse import csr_array\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"kos\", \"nips\", \"nytimes\", \"enron\", \"pubmed\"]\n",
    "\n",
    "docword_paths = [\n",
    "    os.path.join(\"resources\", \"data\", f\"docword.{name}.txt.gz\")\n",
    "    for name in dataset_names\n",
    "]\n",
    "\n",
    "vocab_paths = [\n",
    "    os.path.join(\"resources\", \"data\", f\"vocab.{name}.txt\") for name in dataset_names\n",
    "]\n",
    "\n",
    "vocab_mapping_paths = [\n",
    "    os.path.join(\"resources\", \"data\", f\"{name}_map.pkl\") for name in dataset_names\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docword_path = docword_paths[2]\n",
    "dataset_name = dataset_names[2]\n",
    "vocab_mapping_path = vocab_mapping_paths[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv segmentation\n",
    "\n",
    "spark = SparkSession.builder.appName(\"spark\").getOrCreate()\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"doc_id\", IntegerType(), True),\n",
    "        StructField(\"word_id\", IntegerType(), True),\n",
    "        StructField(\"count\", IntegerType(), True),\n",
    "    ]\n",
    ")\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"delimiter\", True)\n",
    "    .option(\"header\", False)\n",
    "    .option(\"delimiter\", \" \")\n",
    "    .schema(schema)\n",
    "    .load(docword_path)\n",
    ")\n",
    "df = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "df = df.where(df.id > 2)\n",
    "df = df.drop(\"id\")\n",
    "\n",
    "n = df.select(\"doc_id\").distinct().count() // 10000\n",
    "df = df.withColumn(\"part_col\", col(\"doc_id\") % n)\n",
    "temp_path = \"temp\"\n",
    "df.write.format(\"csv\").partitionBy(\"part_col\").save(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_vocab_path = \"resources/data/vocab_unified.csv\"\n",
    "max_word_id = pd.read_csv(unified_vocab_path).shape[0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts all records with word counts of given doc to\n",
    "# Sparse vector\n",
    "def flatten_group(group):\n",
    "    ids = group[\"word_id\"].values\n",
    "    counts = group[\"count\"].values\n",
    "    bow_vector = np.zeros(max_word_id)\n",
    "    bow_vector[ids - 1] = counts\n",
    "    bow_vector = csr_array(bow_vector)\n",
    "    return bow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vocab_mapping_path, \"rb\") as f:\n",
    "    mapping = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(columns=[\"doc_id\", \"bow_vector\"])\n",
    "\n",
    "for d in tqdm(os.listdir(temp_path)):\n",
    "    if not d.startswith(\"part_col=\"):\n",
    "        continue\n",
    "    path = os.path.join(temp_path, d)\n",
    "    csv_path = list(filter(lambda dir: dir.endswith(\".csv\"), os.listdir(path)))[0]\n",
    "    temp_df = pd.read_csv(\n",
    "        os.path.join(path, csv_path),\n",
    "        header=None,\n",
    "    )\n",
    "    temp_df.columns = [\"doc_id\", \"word_id\", \"count\"]\n",
    "    temp_df[\"word_id\"] = temp_df[\"word_id\"].map(mapping)\n",
    "    temp_df = (\n",
    "        temp_df.groupby(\"doc_id\", as_index=False)\n",
    "        .apply(flatten_group)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    temp_df.columns = [\"doc_id\", \"bow_vector\"]\n",
    "    out_df = pd.concat([out_df, temp_df]).reset_index(drop=True)\n",
    "    del [temp_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_pickle(f\"resources/data/converted.{dataset_name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(temp_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40ccfa67bf68b2ed06fcd07ea11f0108ca30e76e3ea29b3f4adfde8e4b3f6a56"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('wum')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
