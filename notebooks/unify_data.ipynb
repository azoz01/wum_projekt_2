{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "In original data indexing is started from 1 whilst in this notebook and corresponding pipeline indexing is stared from 0. It allows to use `pandas.merge` function using only `index` as column and `id` column in joins, e.g:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab['index'] = vocab.index\n",
    "# merged_df = pd.merge(vocab, docword, left_on='index', right_index='vocab_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\n",
    "    \"enron\",\n",
    "    \"kos\",\n",
    "    \"nips\",\n",
    "    # \"nytimes\",\n",
    "    # \"pubmed\"\n",
    "]\n",
    "\n",
    "docword_paths = [\n",
    "    os.path.join(\"resources\", \"data\", f\"docword.{name}.txt.gz\")\n",
    "    for name in dataset_names\n",
    "]\n",
    "\n",
    "vocab_paths = [\n",
    "    os.path.join(\"resources\", \"data\", f\"vocab.{name}.txt\") for name in dataset_names\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_vocab_df(df):\n",
    "    df[\"index\"] = df.index\n",
    "    df.rename(columns={0: \"vocab\"}, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def update_docword(df: pd.DataFrame, mapping: dict):\n",
    "    df.rename(columns={0: \"article_id\", 1: \"old index\", 2: \"count\"}, inplace=True)\n",
    "    df[\"vocab_index\"] = df[df.columns[1]].map(mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: work only if vocab and names paths are passed in the same order\n",
    "\n",
    "def unify_vocab(\n",
    "    vocab_paths,\n",
    "    names,\n",
    "    vocab_out=\"vocab_unified.csv\",\n",
    "):\n",
    "    # read and preapre dfs\n",
    "    dfs = [pd.read_csv(df, header=None) for df in vocab_paths]\n",
    "    dfs = list(map(rename_vocab_df, dfs))\n",
    "\n",
    "    # create and preapre full vocab df\n",
    "    df_concat = reduce(\n",
    "        lambda df1, df2: pd.concat([df1[[\"vocab\"]], df2[[\"vocab\"]]]), dfs\n",
    "    )\n",
    "    df_concat.drop_duplicates(inplace=True)\n",
    "    df_concat[\"index\"] = df_concat.index\n",
    "\n",
    "    df_concat[[\"vocab\"]].to_csv(\n",
    "        path_or_buf=os.path.join(\"resources\", \"data\", f\"{vocab_out}\"),\n",
    "        header=False,\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    # create mapping for indexes\n",
    "    merged_dfs = list(\n",
    "        map(\n",
    "            lambda df: pd.merge(\n",
    "                df_concat,\n",
    "                df,\n",
    "                how=\"inner\",\n",
    "                on=\"vocab\",\n",
    "                suffixes=(\"_merged\", \"_original\"),\n",
    "            )[[\"index_merged\", \"index_original\"]],\n",
    "            dfs,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # NOTE: adding 1 to index is necessary to match numeration from 0 (pandas) and 1 (source files)\n",
    "    index_mapping = [\n",
    "        {\n",
    "            (row[1] + 1): row[0]\n",
    "            for row in df[[\"index_merged\", \"index_original\"]].to_numpy()\n",
    "        }\n",
    "        for df in merged_dfs\n",
    "    ]\n",
    "\n",
    "    # save mappings\n",
    "    for (mapping, name) in zip(index_mapping, names):\n",
    "        with open(os.path.join(\"resources\", \"data\", f\"{name}_map.pkl\"), \"bw\") as f:\n",
    "            pkl.dump(mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: work only if docword and names paths are passed in the same order\n",
    "\n",
    "def update_docwords(docword_paths, names, docword_out_suffix=\"_unified\"):\n",
    "    # creating and preapring docwords for update\n",
    "    docword_dfs = [\n",
    "        pd.read_csv(docword_path, compression=\"gzip\", skiprows=3, header=None, sep=\" \")\n",
    "        for docword_path in docword_paths\n",
    "    ]\n",
    "\n",
    "    index_mapping = []\n",
    "    for name in names:\n",
    "        with open(os.path.join(\"resources\", \"data\", f\"{name}_map.pkl\"), \"rb\") as f:\n",
    "            index_mapping.append(pkl.load(f))\n",
    "\n",
    "    updated_docword_dfs = list(map(update_docword, docword_dfs, index_mapping))\n",
    "\n",
    "    # save updated docwords\n",
    "    for (df, name) in zip(updated_docword_dfs, names):\n",
    "        df.to_csv(\n",
    "            path_or_buf=os.path.join(\n",
    "                \"resources\", \"data\", f\"{name}{docword_out_suffix}.csv\"\n",
    "            ),\n",
    "            header=True,\n",
    "            index=False,\n",
    "            columns=[\"article_id\", \"vocab_index\", \"count\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "unify_vocab(vocab_paths, dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_docwords(docword_paths, dataset_names)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fdf3126e63255e69d3bd26326e0fffc504a640e14fb0bce32d700476ded269b1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('WUM2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
